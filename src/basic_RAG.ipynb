{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "\n",
    "DATA_PATH = \"../data/books\"\n",
    "book = \"Harry-Potter-and-the-Philosophers-Stone\"\n",
    "loader = PyPDFLoader(DATA_PATH + \"/\" + book + \".pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# # Each \"page\" is a LangChain Document with .page_content\n",
    "# for i, page in enumerate(pages[:3]):\n",
    "#     print(f\"\\n--- Page {i + 1} ---\\n\")\n",
    "#     print(page.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUNK THE TEXT\n",
    "\n",
    "chunk_size = 1000\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=500,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "chunks = splitter.split_documents(pages)\n",
    "\n",
    "page = chunks[50]\n",
    "print(page.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE VECTOR DATABASE TO EMBED EACH OF THE CHUNKS\n",
    "\n",
    "# Free model from hugging face\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create vector database by embedding each of the chunks using the\n",
    "#  specified embedding model\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "# # Remove previous database if making a new one\n",
    "# if os.path.exists(CHROMA_PATH):\n",
    "#     shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "# # Create vector database\n",
    "# db = Chroma.from_documents(chunks, embedding_function, persist_directory=CHROMA_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "\n",
    "# Load the database\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "# Search the database\n",
    "query_text = \"Who is Hermione Granger?\"\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "# # Filter\n",
    "# if len(results) == 0 or results[0][1] < 0.5:\n",
    "#     print(\"Unable to find good results\")\n",
    "\n",
    "# View the retrieval\n",
    "retrieval = \"\\n\\n---\\n\\n\".join([page.page_content for page, _ in results])\n",
    "print(retrieval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE RESPONSE\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful book assistant. Given the following excerpts from a novel, answer the userâ€™s question as clearly and concisely as possible, using only the provided text.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=retrieval, query=query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"mistral-7b-instruct.Q4_K_M.gguf\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    n_ctx=2048,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
